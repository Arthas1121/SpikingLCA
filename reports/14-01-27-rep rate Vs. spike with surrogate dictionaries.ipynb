{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 14-01-27 report\n",
      "\n",
      "Same as previous report, but now I don't create a random G, but rather random dictionaries and random input vectors, which I use to construct G and $i_{stim}$. The report is composed of two parts:\n",
      "\n",
      "* Comparing the 2 different rate-based systems\n",
      "* Exploring the spiking model implementation"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Comparison of 2 rate-based systems \n",
      "\n",
      "As neurons can not have a negative activation values (they either spike with a positive freq. or they don't), we might want to think to implement the limited LCA, where we only allow for the positive activation values. It is unclear to me how this disturbs the LCA, so I compare the two implementations. I force the positive activation weights by modifying the hard thresholding function.\n",
      "\n",
      "### The implementation\n",
      "\n",
      "As previously, for a rate system, we take the LCA set of equations:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\tau \\dot{u}_i(t) &= -u_i(t) - \\sum_{j}(\\Phi_i^T\\Phi_j-\\delta_{i,j}) a_j(t) + \\Phi_i^T x_i\\\\\n",
      "a_i &= T_{\\lambda}(u_i),\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "where x is the input to the system and $\\Phi_i$ a dictionary to which the node $u_i$ corresponds. This time I generate surrogate input vector ($x$) and surrogate dictionaries ($\\Phi_i$). Having those, the system can be written as:\n",
      "\n",
      "$$\n",
      "\\begin{align}\n",
      "\\tau \\dot{u}(t) &= -u(t) - G a(t) + b\\\\\n",
      "a_i &= T_{\\lambda}(u_i),\n",
      "\\end{align}\n",
      "$$\n",
      "\n",
      "where $b_m=\\Phi_m^T x$. T(.) is a hard threshold function which is different for the two cases:\n",
      "\n",
      "* **Case 1: the thresholding function is symmetrical (with negative)** It is different from the previous report in such, that the slope is now 1 for the u values about the threshold. Previously this would cause instabilities, as I was constructing G randomly.\n",
      "* **Case 2: the thresholding function is not symmetrical (non-negative)** and is constructed in a way that it does not allow the negative weights. This is more like what happens in LIF neurons.\n",
      "\n",
      "![caption](files/figures/hard-thresholding-function2.png)\n",
      "\n",
      "### The results\n",
      "\n",
      "First I look for how the sparsness depends on the threshold for the two cases.\n",
      "\n",
      "![caption](files/figures/sparsness_on_threshold_both.png)\n",
      "\n",
      "It seems both cases are very similar in this respect. Now we explore the error of reconstruction, where we define the error simply as the euclidian distance between the input vector $x$ and the reconstructed vector $\\Phi a$.\n",
      "\n",
      "![caption](files/figures/error_on_sparsness_both.png)\n",
      "\n",
      "We can clearly see the \"non-negative\" case performs worse then the \"with negative\" case:\n",
      "\n",
      "* The non-negative case can exactly reconstruct the input vector (although the representation in this case is not sparser). \n",
      "* There is a certain loss of precision (compared to the with negative case) as we increase the sparsness."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Spiking system\n",
      "\n",
      "The spiking system tries to mimic the LCA rate model activity, where given the constraint of non-negaitve activation vectors, it should resemble more to the non-negative rate model described in the previous section.\n",
      "\n",
      "### Methods\n",
      "#### Implementation\n",
      "The spiking alternative to the rate-based dynamic system is to have the nodes exist as conductance-based LIF neurons and connect them to each other using the G matrix. NOTE: Given the structure of G, this means that the neurons are symetrically connected and can be both excitatory as well as inhibitory.\n",
      "\n",
      "Each neuron also recieves the stimulation current which is proporitonal to $b_i$.\n",
      "\n",
      "The equation governing the dynamics of the single node are:\n",
      "\n",
      "\\begin{align}\n",
      "\\tau_m \\dot{v_m}(t) &= (V_R-v_m(t)) + g_e(V_E-v_m(t))+ g_i(V_I-v_m(t)) + i_{stim}\\\\\n",
      "\\tau_e\\frac{dg_e(t)}{dt}&=-g_e + w_e\\sum_i{\\delta (t-t_i)}\\\\\n",
      "\\tau_i\\frac{dg_i(t)}{dt}&=-g_i + w_i\\sum_j{\\delta (t-t_j)}\\\\\n",
      "\\tau_a\\frac{da(t)}{dt}&=-a + \\frac{1}{\\tau_a}\\sum_k{\\delta (t-t_k)}\\\\\n",
      "\\end{align}\n",
      "\n",
      "where $v_m$ corresponds to the membrane voltage, $g_e and g_i$ are the cumulative excitatory and inhibitory conductance the neuron is exposed to respectively. As soon as the $v_m$ reaches the threhold value ($V_{thr} = -50 mV$) it emits a spike and the membrane voltage is reset to the resting value ($V_R = -60 mV$) and clamped for the refrectory time ($\\tau_{ref}=5ms$). The spike is propagated to all the other neurons with the weight corresponging to the matrix G. If the weight is positive, the spike causes an increase in the cumulative excitatory conductance ($g_e$) of the postsynaptic neuron. If it is negative, it casues an increase in the cumulative inhibitory conductance ($g_i$).\n",
      "\n",
      "The equation involing $a$ is not really a part of the system dynamics, it is only an attempt to get the continuous value from the spikes by convlving them with the exponential kernel.\n",
      "\n",
      "#### Setting the parameters\n",
      "\n",
      "To get the parameters for spiking network based on the rate-based system, we use:\n",
      "\n",
      "\\begin{align}\n",
      "i_{stim}^{(i)} &= \\alpha b_i\\\\\n",
      "\\end{align}\n",
      "\n",
      "$$\n",
      "w_e^{(i,j)}= \n",
      "\\begin{cases}\n",
      "    -\\beta G_{i,j},& \\text{if } G_{i,j} < 0\\\\\n",
      "    0,              & \\text{otherwise}\n",
      "\\end{cases}\n",
      "$$\n",
      "$$\n",
      "w_i^{(i,j)}= \n",
      "\\begin{cases}\n",
      "    \\gamma G_{i,j},& \\text{if } G_{i,j} > 0\\\\\n",
      "    0,              & \\text{otherwise}\n",
      "\\end{cases}\n",
      "$$\n",
      "\n",
      "There are thus at least 3 parameters you can play around with ($\\alpha$, $\\beta$ and $\\gamma$). I chose the parameter $\\alpha$ such that the $\\|i_{stim}\\|=1$. The ration between $\\gamma$ and $\\beta$ is set such that the net excitation and net inhibition in the system are balanced. As the mean ration between the driving potential of excitation ($|0mV - (-60)mV| = 60 mV$) and the driving potential of inhibition ($|-80mV - (-60)mV| = 20mV$) is 3, I set $\\beta=\\gamma/3$ to compensate.\n",
      "\n",
      "\n",
      "Having fixed those relationships, the ration between $\\alpha$ and $\\beta$ give the ratio between how much the dynamics is influenced by the input and how much by the recurrent connections. I vary this ration in the following analysis.\n",
      "\n",
      "#### Calculating the error \n",
      "\n",
      "As in the rate case I take the euclidian distance between the input vector ($x$) and the reconstruction vector ($\\Phi a$) as the reconstruction error, but this time I resize the reconstruction vectors to the length that yields the smallest error. I believe this is also what happens in rate-based LCA. To get the optimized reconstruction vector $(\\Phi a)_{op}$, I thus take:\n",
      "$$\n",
      "(\\Phi a)_{op} = \\langle \\Phi a, x/\\|x\\|\\rangle \\Phi a\n",
      "$$\n",
      "\n",
      "The mean error for taking a random vector and optimize it's length for our dimensionality is about 0.964. NOTE: When we rescale the vector length, we might also invert it's direction (scaling factor can be negative) if this would cauese a better fit.\n",
      "\n",
      "### Results\n",
      "\n",
      "* A bit obvious, but neurons can not give negative activation values (a) - they are only positive.\n",
      "* I get to some overlap between the active neurons in the spiking and the rate-based case if I tune the parameters right (3 out of 5) - not shown here.\n",
      "* I look at the three cases of different interconnection strength:\n",
      " \n",
      "#### Medium strength of connections\n",
      "\n",
      "![caption](files/figures/spike-reconstruction-error.png)\n",
      "\n",
      "* I can get the error down to somewhere around 0.5 for a reasonable sparsness (8 out of 64). (NOTE: the max. error is 2, mean would be around 0.96)\n",
      "* I can get the error down to 0.1 or so, but that would be some sort of overfitting already, as we use about 42 active nodes to represent a vector of dimension 16.\n",
      "* When we maximally lower the threshold, about 80% of the nodes are active. At that point the reconstruction error is rather big (as good as random).\n",
      "\n",
      "#### Weak strength of connections (0.1 times the medium)\n",
      "\n",
      "![caption](files/figures/spike-reconstruction-error_weak.png)\n",
      "\n",
      "* I can get the error down to somewhere around 0.65 for a reasonable sparsness (8 out of 64).\n",
      "* I can get the error down to 0.5 or so, but that would be some sort of overfitting already, as we use about 30 active nodes to represent a vector of dimension 16.\n",
      "* When we maximally lower the threshold, about 40% of all the nodes are active.\n",
      "\n",
      "#### Strong strength of connections (10 times the medium)\n",
      "\n",
      "![caption](files/figures/spike-reconstruction-error_strong.png)\n",
      "\n",
      "* I can not get a reasonable sparsness at all (I expect due the to big fluctuations of the membrane voltage). I either have about 80% of the nodes active, or 1 node or none. Good example of how overconnectivity can kill your network usability.\n",
      "\n",
      "### Conclusions\n",
      "\n",
      "* There is a sweet spot for the strength of the interconnectivity of the neurons and the threshold. I haven't really been optimizing it much or explore how this sweet spot depends on the input vector.\n",
      "* The best case of sparse reconstruction I could get to (with very limited parameter sweep and no stability analysis) had an error of 0.5 which is about 2 times worse then the comparable rate-based system and 2 times better then a random reconstruction."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Next steps\n",
      "\n",
      "- Go for a LCA model which is used in PetaVision - splitting of V1 and LGN and passing of the reconstruction error.\n",
      "- Reduce the number of parameters that need manual setting by finding a more principled way of setting them.\n",
      "- Decide what to do with the negitive coefficients (= architecture of the system). "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}