{
 "metadata": {
  "name": "14-02-04 report - spike3-rate4 comparison"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# 14/02/04 report - comparions of spike3 and rate4 alghorithms\n",
      "\n",
      "## Content:\n",
      "\n",
      "* Literature search\n",
      "* Soft Vs. Hard threhsold exploration\n",
      "* I-f curve of the individual neuron as the threshold function (general explorations)\n",
      "* Spike Vs. Rate comparison (positive only inputs - using generative model for surrogate data)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Literature search\n",
      "\n",
      "I found two quite relevant papers:\n",
      "\n",
      "* [Shapero, Rozell Hasler, 2013 - Configurable hardware integrate and fire neurons for sparse approximation.](http://www.ncbi.nlm.nih.gov/pubmed/23582485)  \n",
      "   They not only implement the LCA in the spiking configuration, but they have it in hardware as well. \n",
      "  \n",
      "    * Instead of LIF they use IF neurons, so they have a linear IF curve.\n",
      "    * They shift the IF curve by $\\lambda$ to get the soft threshold criterion.\n",
      "    * They only use the positive inputs and the positive dictionary elements - they input the LPF signal of the synaptic activity (negative only).\n",
      "    * They have a set of 18 dict. elements, for a 12 dimensional input and they test the algorithm on k=1-4 active elements only. Pretty limiting comparison.\n",
      "  \n",
      "* [Hu, Genkin, Chkolovskii, 2012 - A Network of Spiking Neurons for Computing Sparse\n",
      " Representations in an Energy-Efficient Way](http://www.ncbi.nlm.nih.gov/pubmed/22920853)\n",
      "\n",
      "    * Coming up with a new LCA-inspired algorithm which they name HDA (hybrid distributed algorithm)\n",
      "    * Replace the L1 norm with something called the Bergman divergence.\n",
      "    * No hardware, but tested on the realistic vision problems.\n",
      "    * The IF solution only described, in their solution they have negative spiking.\n",
      "    * Quite intensive math involved, I have not understood it fully yet.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Soft Vs. Hard threshold exploration\n",
      "\n",
      "I wanted to see what is up with hard or soft thresholds in rate-based LCA. From what I could see the hard threshold definitely outpreforms the soft one. In the soft threshold I also need to optimize the output vector size, to get the optimal error. This is not really required in the hard threshold case.\n",
      "\n",
      "![caption](files/figures/thresholds.png)\n",
      "![caption](files/figures/hard_Vs_soft_performance.png)\n",
      "\n",
      "It seems the hard threshold generally outperforms the soft threshold."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## I-f curve of the individual neuron as the threshold function"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The thresholding function $T(.)$ in the spiking network is achieved by the f-I curve, the translation between the input currents ($I$) to the neuron and its firing frequency ($f$). As this is one of the key elements of LCA, I looked into it more in detail. Couple of findings:\n",
      "\n",
      "* Both input types (current and synaptic inputs) have qualitatively same f-I curve (or at least very a very similar ones).\n",
      "* The slope of the curve after the threshold value is never quite linear. There is some amount of sublinearity. present. And of course there would be the saturation at values determined by the refractory period.\n",
      "* Changing the membrane threshold value not only changes the min requried input for spiking (the equivalent of the threshold in the LCA model), but it also changes the slope of the curve.\n",
      "\n",
      "![caption](files/figures/f-I_curve/f-I_on_t_ref_spiking-stim.png)\n",
      "![caption](files/figures/f-I_curve/f-I_on_threshold_curr-stim.png)\n",
      "![caption](files/figures/f-I_curve/f-I_on_w_e_spiking-stim.png)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Spike Vs. Rate comparison\n",
      "\n",
      "I chose the parameter settings on the spiking model that correspond to the most linear possible function I could get my hands on and ran a comparison between the spiking and the rate model (soft threhold and inputs are positive) for some different dimensions.\n",
      "\n",
      "![caption](files/figures/spiking3-rate4-comparison.png)\n",
      "\n",
      "It turns out the spiking implementation is on par with the rate implementation up until 5-8 neurons are active. After this the rate-based LCA slowly converges towards 0 error while the spiking-based LCA can never get below a certain \"cieling\" error. In fact typically as you would have more neurons active, the error starts increasing."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Next steps\n",
      "\n",
      "* Figure out where the discrepancies occur between the spiking and rate model by studying the rate model with the threshold that exactly corresponds to the fI curve of the spiking network. (2 days)\n",
      "\n",
      "* Test out the spiking network on only the positive dictionary elements. (2 days)\n",
      "\n",
      "* Implement the new measure of sparsness for the spiking networks - total rate of the network. (1 day) \n",
      "\n",
      "* Come up with the information theory measure which corresponds to the fidelity of the reconstruction. How can we say that each spike carries so and so much of information? (1 week)\n",
      "\n",
      "* Start using Petavision for the implementation of spiking LCA. First step, get Petavision compiled and running on your system and try out the tests. (1 day)"
     ]
    }
   ],
   "metadata": {}
  }
 ]
}